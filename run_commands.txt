==== Run commands examples ====

---- Finetune ----

export MODEL=mBART_large_50
# export MODEL=mT5_base
export LANG=arabic
export TUNE=finetune
export SEED=123
export LR=5e-5
export EPOCH=3

nohup python train.py --seed ${SEED} --cfg xlsum/${MODEL}/${TUNE}/${LANG}.cfg --run_name ${MODEL}_${TUNE}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 10000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 10000 --save_total_limit 2 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true > logs/${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR}.log 2>&1


---- LoRA ----

export MODEL=mBART_large_50
# export MODEL=mT5_base
export LANG=arabic
export TUNE=lora
export SEED=123
export LR=5e-5
export EPOCH=3

nohup python train.py --seed ${SEED} --cfg xlsum/${MODEL}/${TUNE}/${LANG}.cfg --run_name ${MODEL}_${TUNE}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 100000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 100000 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true > logs/${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR}.log 2>&1


---- Prefix-tuning 100 ----

export MODEL=mBART_large_50
# export MODEL=mT5_base
export LANG=chinese_simplified
export TUNE=prefix
export SEED=123
export LR=5e-5
export EPOCH=3

nohup python train.py --seed ${SEED} --cfg xlsum/${MODEL}/${TUNE}/${LANG}.cfg --run_name ${MODEL}_${TUNE}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 100000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 100000 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true > logs/${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR}.log 2>&1


---- Prefix 30, 50, 200, 300, 400 ----

export MODEL=mBART_large_50
export LANG=chinese_simplified
export TUNE=prefix
export SEED=123
export LR=5e-5
export PREFIXLEN=200
export EPOCH=3

nohup python train.py --seed ${SEED} --cfg xlsum/${MODEL}/${TUNE}/${PREFIXLEN}/${LANG}.cfg --run_name ${MODEL}_${TUNE}${PREFIXLEN}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 100000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 100000 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/${MODEL}_${TUNE}${PREFIXLEN}_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true > logs/${MODEL}_${TUNE}${PREFIXLEN}_${LANG}_seed${SEED}_lr${LR}.log 2>&1


==== Few-shot Experiments ====

---- Finetune ----

export MODEL=mBART_large_50
export LANG=arabic
export TUNE=finetune
export SEED=123
export LR=5e-5
export EPOCH=3

nohup python train.py --seed ${SEED} --cfg few_shot_xlsum/${MODEL}/${TUNE}/${LANG}.cfg --run_name few_shot_${MODEL}_${TUNE}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 10000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 10000 --save_total_limit 2 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/few_shot_${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true > logs/few_shot_${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR}.log 2>&1


---- Prefix ----

export MODEL=mBART_large_50
export LANG=chinese_simplified
export TUNE=prefix
export SEED=123
export LR=5e-5
export EPOCH=3

nohup python train.py --seed ${SEED} --cfg few_shot_xlsum/${MODEL}/${TUNE}/${LANG}.cfg --run_name ${MODEL}_${TUNE}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 100000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 100000 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/few_shot_${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true > logs/few_shot_${MODEL}_${TUNE}_${LANG}_seed${SEED}_lr${LR}.log 2>&1



==== DEBUG ====

export LANG=chinese_simplified
export TUNE=lora
export SEED=123
export LR=5e-5
export EPOCH=1

python train.py --seed ${SEED} --cfg xlsum/test/test_mBART_large_50_${TUNE}_xlsum_${LANG}.cfg --run_name test_mBART_large_50_${TUNE}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 100000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 100000 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 4 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/test_mBART_large_50_${TUNE}_xlsum_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true

python train.py --seed ${SEED} --cfg xlsum/test/test_mT5_base_${TUNE}_xlsum_${LANG}.cfg --run_name test_mT5_base_${TUNE}_${LANG} --logging_strategy steps --logging_first_step true --logging_steps 1000 --evaluation_strategy steps --eval_steps 100000 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 100000 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 8 --num_train_epochs ${EPOCH} --adafactor true --learning_rate ${LR} --do_train --do_eval --do_predict --predict_with_generate --output_dir output/test_mT5_base_${TUNE}_xlsum_${LANG}_seed${SEED}_lr${LR} --overwrite_output_dir --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --generation_num_beams 4 --generation_max_length 60 --input_max_length 1024 --ddp_find_unused_parameters true
