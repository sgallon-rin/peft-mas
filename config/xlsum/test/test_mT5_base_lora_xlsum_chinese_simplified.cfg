[model]
name = unified.peft
use_description = False
concatenate_description = False
knowledge_usage = summarization
#src_lang = zh_CN  # mT5 does not require language code
#tgt_lang = zh_CN

[peft]
peft_type = LORA
lora_r = 16
lora_alpha = 32
lora_dropout = 0.05
;peft_type = PREFIX_TUNING
;num_virtual_tokens = 200

[dataset]
data_store_path = ./data

[seq2seq]
constructor = seq2seq_construction.meta_tuning

[arg_paths]
xlsum = META_TUNING/test/test_xlsum_chinese_simplified.cfg

[evaluate]
tool = metrics.meta_tuning.evaluator

[bert]
location = google/mt5-base